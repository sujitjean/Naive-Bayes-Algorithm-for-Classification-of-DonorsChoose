# Naive-Bayes-Algorithm-for-Classification-of-DonorsChoose
In the journey of exploring the flied of Data science and predictivemodeling, I explored this Very interesting algorithm Naive Bayes algorithm. I havetried to leverage the ability of the Classification algorithm which comes underSupervised learning of Section of predictive modeling. I used the Naive Bayes algorithm for the classification ofapproval rate of the projects submitted by the teachers of United states for students.The main business context of the Project was to reduce the manualevaluation of the projects that was done by volunteers as the process of evaluationcan take long time, which may also be biased on some factors and some irreducibleerrors could also be introduced into the processes. Some other import points are.·      How to scale current manual processes andresources to screen 500,000 projects so that they can be posted as quickly andas efficiently as possible·      How to increase the consistency of projectvetting across different volunteers to improve the experience for teachers·      How to focus volunteer time on the applicationsthat need the most assistance.The goal of the Project is to predict whether or not aDonorsChoose.org project proposal submitted by a teacher will be approved,using the text of project descriptions as well as additional metadata about theproject, teacher, and school. DonorsChoose.org can then use this information toidentify projects most likely to need further review before approvalThe steps followed for Data Preparation and PredictiveModeling is as follows:Note: Giving Unstructured data (Garbage in common terms) toa machine learning algorithm gives you random data (Garbage) again. All the code is written in a very clean and untestablemanner ignoring fancy methods where ever possible and reference for everything thatis used in coding is given above the code so that is it easy for everyone tounderstand the code and leverage the potential that AI has, because I believein growing together and helping others as this makes me a great team player . Italso increases the story telling ability and to represent data.For implementation of all the code I have used the SKlearn Library.  1.      Applying Multinomial NB on these featuresetsSet 1: categorical, numericalfeatures + preprocessed_eassay (BOW)Set 2: categorical, numericalfeatures + preprocessed_eassay (TFIDF)2.      The hyper parameter tuning(find best alpha:smoothing parameter)1.      Find the best hyper parameter which will givethe maximum AUC value.2.      find the best hyper parameter using k-fold crossvalidation (use GridsearchCV or RandomsearchCV)/simple cross validation data(write for loop to iterate over hyper parameter values)3.       Representationof results1.      plot the performance of model both on train dataand cross validation data for each hyper parameter, like shown in the figure 2.      Once after you found the best hyper parameter,you need to train your model with it, and find the AUC on test data and plotthe ROC curve on both train and test. 3.      Along with plotting ROC curve, you need to printthe confusion matrix with predicted and original labels of test data points 4.      The top 20 features from either from feature Set1 or feature Set 2 using absolute values of `feature_log_prob_ ` parameter of`MultinomialNB` (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)and print their corresponding feature names      4.  You need to summarize the results at the endof the notebook, summarize it in the table format 
